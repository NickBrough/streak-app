// Expo config plugin to wire up an iOS VisionCamera pose detector that
// uses Google ML Kit and exposes a `detectPose` frame processor for
// react-native-vision-camera.
//
// This plugin:
// - Ensures the ML Kit pod is added to the Podfile
// - Writes `PoseDetector.swift` and `PoseDetectorFrameProcessorPlugin.mm`
//   into `ios/streakapp/`
// - Adds those files to the Xcode target so they are compiled

const fs = require("fs");
const path = require("path");
const {
  withDangerousMod,
  withXcodeProject,
  withPodfile,
} = require("@expo/config-plugins");

const IOS_APP_NAME = "streakapp";

const SWIFT_FILENAME = "PoseDetector.swift";
const MM_FILENAME = "PoseDetectorFrameProcessorPlugin.mm";

const SWIFT_CONTENT = `//
// Auto-generated by Expo config plugin
// Pose Detection module for VisionCamera frame processor
//

import Foundation
import MLKitPoseDetection
import MLKitVision
import VisionCamera

@objc(PoseDetector)
class PoseDetector: NSObject {
  private static var sharedInstance: PoseDetector?
  private let poseDetector: MLKitPoseDetection.PoseDetector
  
  override init() {
    // Use accurate pose detector for better quality
    let options = AccuratePoseDetectorOptions()
    options.detectorMode = .stream
    self.poseDetector = PoseDetector.poseDetector(options: options)
    super.init()
  }
  
  @objc static func requiresMainQueueSetup() -> Bool {
    return false
  }
  
  @objc static func shared() -> PoseDetector {
    if sharedInstance == nil {
      sharedInstance = PoseDetector()
    }
    return sharedInstance!
  }
  
  @objc func detect(_ frame: Frame) -> [String: Any]? {
    guard let pixelBuffer = frame.pixelBuffer else {
      return nil
    }
    
    // Create VisionImage from pixel buffer
    let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
    let context = CIContext()
    guard let cgImage = context.createCGImage(ciImage, from: ciImage.extent) else {
      return nil
    }
    
    let visionImage = VisionImage(image: UIImage(cgImage: cgImage))
    visionImage.orientation = .up
    
    // Perform pose detection synchronously (in worklet context)
    var detectedPose: Pose? = nil
    var detectionError: Error? = nil
    let semaphore = DispatchSemaphore(value: 0)
    
    poseDetector.process(visionImage) { poses, error in
      if let error = error {
        detectionError = error
        semaphore.signal()
        return
      }
      
      // Get the first (most confident) pose
      detectedPose = poses?.first
      semaphore.signal()
    }
    
    // Wait for detection with timeout (100ms max)
    _ = semaphore.wait(timeout: .now() + 0.1)
    
    guard let pose = detectedPose else {
      return nil
    }
    
    // Convert pose landmarks to dictionary format matching our TypeScript Pose type
    var landmarks: [String: [String: Any]] = [:]
    
    // Map ML Kit landmark types to our string keys
    let landmarkMap: [(PoseLandmarkType, String)] = [
      (.nose, "nose"),
      (.leftEyeInner, "left_eye"),
      (.rightEyeInner, "right_eye"),
      (.leftShoulder, "left_shoulder"),
      (.rightShoulder, "right_shoulder"),
      (.leftElbow, "left_elbow"),
      (.rightElbow, "right_elbow"),
      (.leftWrist, "left_wrist"),
      (.rightWrist, "right_wrist"),
      (.leftHip, "left_hip"),
      (.rightHip, "right_hip"),
      (.leftKnee, "left_knee"),
      (.rightKnee, "right_knee"),
      (.leftAnkle, "left_ankle"),
      (.rightAnkle, "right_ankle"),
    ]
    
    let frameWidth = CGFloat(frame.width)
    let frameHeight = CGFloat(frame.height)
    
    for (type, key) in landmarkMap {
      let landmark = pose.landmark(ofType: type)
      let position = landmark.position
      
      // Normalize coordinates to 0-1 range
      let x = Double(position.x / frameWidth)
      let y = Double(position.y / frameHeight)
      let z = Double(landmark.position3D?.z ?? 0.0)
      let score = Double(landmark.inFrameLikelihood)
      
      landmarks[key] = [
        "x": x,
        "y": y,
        "z": z,
        "score": score,
      ]
    }
    
    return ["landmarks": landmarks]
  }
}

// Global registration for VisionCamera frame processor
@objc class PoseDetectorModule: NSObject {
  @objc static func register() {
    // Ensure the shared instance is created
    let _ = PoseDetector.shared()
  }
}
`;

const MM_CONTENT = `//
// VisionCamera Frame Processor Plugin for Pose Detection
// This exposes the PoseDetector to the worklet context.
//

#import <Foundation/Foundation.h>
#import <VisionCamera/FrameProcessorPlugin.h>
#import <VisionCamera/Frame.h>
#import "streakapp-Swift.h"

@interface PoseDetectorFrameProcessorPlugin : NSObject
@end

@implementation PoseDetectorFrameProcessorPlugin

static inline id detectPose(Frame* frame, NSArray* arguments) {
  @autoreleasepool {
    // Get the shared PoseDetector instance
    PoseDetector* detector = [PoseDetector shared];
    if (!detector) {
      return nil;
    }
    
    // Call detect method
    NSDictionary* result = [detector detect:frame];
    return result;
  }
}

VISION_EXPORT_FRAME_PROCESSOR(detectPose)

@end
`;

function ensureIosFiles(config) {
  return withDangerousMod(config, ["ios", (config) => {
    const iosRoot = config.modRequest.platformProjectRoot; // <project>/ios
    const appDir = path.join(iosRoot, IOS_APP_NAME);

    if (!fs.existsSync(appDir)) {
      fs.mkdirSync(appDir, { recursive: true });
    }

    const swiftPath = path.join(appDir, SWIFT_FILENAME);
    const mmPath = path.join(appDir, MM_FILENAME);

    fs.writeFileSync(swiftPath, SWIFT_CONTENT);
    fs.writeFileSync(mmPath, MM_CONTENT);

    return config;
  }]);
}

function ensureIosPods(config) {
  // We now add the ML Kit pod directly in the Podfile, so this hook is a no-op
  // to avoid accidentally corrupting the Podfile with escape sequences.
  return config;
}

function ensureXcodeSources(config) {
  return withXcodeProject(config, (config) => {
    const project = config.modResults;

    const swiftFile = `${IOS_APP_NAME}/${SWIFT_FILENAME}`;
    const mmFile = `${IOS_APP_NAME}/${MM_FILENAME}`;

    const firstTarget = project.getFirstTarget && project.getFirstTarget();
    const target = firstTarget && firstTarget.uuid;
    if (!target) {
      return config;
    }

    // Add Swift & ObjC++ files as regular source files (not "Plugins")
    // to avoid relying on a Plugins group that may not exist yet.
    project.addSourceFile(swiftFile, { target, plugin: false });
    project.addSourceFile(mmFile, { target, plugin: false });

    return config;
  });
}

const withPoseDetector = (config) => {
  config = ensureIosFiles(config);
  config = ensureIosPods(config);
  // NOTE: We skip automatic Xcode source wiring here because some versions
  // of the xcode project library expect a Plugins group that may not exist
  // in fresh Expo projects, causing prebuild to fail. The generated files
  // will live under ios/streakapp/, and you can add them to the Xcode
  // target once via Xcode if needed.
  return config;
};

module.exports = withPoseDetector;




